{
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "name": "python", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11"
        }, 
        "kernelspec": {
            "display_name": "Python 2 with Spark 1.6", 
            "name": "python2", 
            "language": "python"
        }
    }, 
    "nbformat_minor": 1, 
    "cells": [
        {
            "metadata": {}, 
            "source": "# Introduction to NLP: Basic Concepts\n\nThis notebook guides you through the basic concepts to start working with Natural Language Processing, including how to set up your environment, create and analyze data sets, and work with data files.\n\nThis notebook uses NLTK, a python framework for Natural Language Processing. Some knowledge of Python is recommended. \n\nIf you are new to notebooks, here's how the user interface works: [Parts of a notebook](http://datascience.ibm.com/docs/content/analyze-data/parts-of-a-notebook.html)", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## About Natural Language Processing\n\nNatural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora. Challenges in natural language processing frequently involve natural language understanding, natural language generation (frequently from formal, machine-readable logical forms), connecting language and machine perception, dialog systems, or some combination thereof.\n\n<img src='http://web.stanford.edu/class/cs224n/images/treeFrontSentiment.png' width=\"50%\" height=\"50%\"></img>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "<h2 style=\"text-align:center\"> Topics during this presentation: <h2>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "<section style=\"font-size:18px\">\n<ul>\n<li>Basic NLP operations \n    <ul><li>Tokenization</li>\n        <li>Part of Speech (POS)</li>\n        <li>Chunking </li>\n        <li>Parsing</li>\n    </ul>\n</li>\n<li>Text Classification</li>\n<li>Sentiment Analysis</li>\n</ul>\n</section>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## Introduction", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "### The complexity:  \n\nAmbiguity inherited in human language. Different words with same meaning, same word with different meanings,  non deterministic and non computable grammars, difficulty of represent in the computer the real world semantic.\n", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## NLP Methods", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "- Automatic Summarization\n- Bots! Pew bot, Hubbot! image me drunk.\n- Translations\n- Named Entity Recognition\n- Natural Language generation\n- Optical Character Recognition (OCR)\n- Part of Speech tagging (POS)\n- Parsing\n- Question Answering\n- Sentiment Analysis\n- Speech Recognition\n- Word sense disambiguation\n- Information Retrieval\n- Stemming\n- Heaps More! \n", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## Preprocessing\n\nText is messy data, various types of noise are present in it and the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as text preprocessing.\n\nIt is predominantly comprised of three steps:\n\n - Noise Removal\n - Normalization\n - Standardization\n", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "### Noise Removal\n\nAny piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n\nFor example \u2013 language stopwords (commonly used words of a language \u2013 is, am, the, of, in etc), URLs or links, social media entities (mentions, hashtags), punctuations and industry specific words. This step deals with removal of all types of noisy entities present in the text.\n\nA general approach for noise removal is to prepare a dictionary of noisy entities, and iterate the text object by tokens (or by words), eliminating those tokens which are present in the noise dictionary.\n\nFollowing is the python code for the same purpose.\n\n#### Stopword Filtering", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "noise_list = [\"is\", \"a\", \"this\", \"...\"] \ndef _remove_noise(input_text):\n    words = input_text.split() \n    noise_free_words = [word for word in words if word not in noise_list] \n    noise_free_text = \" \".join(noise_free_words) \n    return noise_free_text\n\n_remove_noise(\"this is a sample text\")", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "### Regex Filtering\n\nAnother approach is to use the regular expressions while dealing with special patterns of noise. We have explained regular expressions in detail in one of our previous article. Following python code removes a regex pattern from the input text:", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "import re \n\ndef _remove_regex(input_text, regex_pattern):\n    urls = re.finditer(regex_pattern, input_text) \n    for i in urls: \n        input_text = re.sub(i.group().strip(), '', input_text)\n    return input_text\n\nregex_pattern = \"#[\\w]*\"  \n\n_remove_regex(\"remove this #FloridaBlue from tweet text\", regex_pattern)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "### Normalization\n\nAnother type of textual noise is about the multiple representations exhibited by single word.\n\nFor example \u2013 \u201cplay\u201d, \u201cplayer\u201d, \u201cplayed\u201d, \u201cplays\u201d and \u201cplaying\u201d are the different variations of the word \u2013 \u201cplay\u201d, Though they mean different but contextually all are similar. The step converts all the disparities of a word into their normalized form (also known as lemma). Normalization is a pivotal step for feature engineering with text as it converts the high dimensional features (N different features) to the low dimensional space (1 feature), which is an ideal ask for any ML model.\n\nThe most common lexicon normalization practices are :\n\n    Stemming:  Stemming is a rudimentary rule-based process of stripping the suffixes (\u201cing\u201d, \u201cly\u201d, \u201ces\u201d, \u201cs\u201d etc) from a word.\n    Lemmatization: Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n\nBelow is the sample code that performs lemmatization and stemming using python\u2019s popular library \u2013 NLTK.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "from nltk.stem.wordnet import WordNetLemmatizer \nlem = WordNetLemmatizer()\n\nfrom nltk.stem.porter import PorterStemmer \nstem = PorterStemmer()\n\nword = \"multiplying\" \nlem.lemmatize(word, \"v\") \nstem.stem(word)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "### Standardization\n\nText data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by search engines and models.\n\nSome of the examples are \u2013 acronyms, hashtags with attached words, and colloquial slangs. With the help of regular expressions and manually prepared data dictionaries, this type of noise can be fixed, the code below uses a dictionary lookup method to replace social media slangs from a text.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "translation_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\"}", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "attachments": {}, 
            "metadata": {}, 
            "source": "## Parsing\n\nSyntactical parsing involves the analysis of words in the sentence for grammar and their arrangement in a manner that shows the relationships among the words. Dependency Grammar and Part of Speech tags are the important attributes of text syntactics.\n\nDependency Trees \u2013 Sentences are composed of some words sewed together. The relationship among the words in a sentence is determined by the basic dependency grammar. Dependency grammar is a class of syntactic text analysis that deals with (labeled) asymmetrical binary relations between two lexical items (words). Every relation can be represented in the form of a triplet (relation, governor, dependent). For example: consider the sentence \u2013 \u201cBills on ports and immigration were submitted by Senator Brownback, Republican of Kansas.\u201d The relationship among the words can be observed in the form of a tree representation as shown:  \n\n<img src='https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/01/11181146/image-2.png' width=\"50%\" height=\"50%\"></img>\n\nThe tree shows that \u201csubmitted\u201d is the root word of this sentence, and is linked by two sub-trees (subject and object subtrees). Each subtree is a itself a dependency tree with relations such as \u2013 (\u201cBills\u201d <-> \u201cports\u201d <by> \u201cproposition\u201d relation), (\u201cports\u201d <-> \u201cimmigration\u201d <by> \u201cconjugation\u201d relation).\n\nThis type of tree, when parsed recursively in top-down manner gives grammar relation triplets as output which can be used as features for many nlp problems like entity wise sentiment analysis, actor & entity identification, and text classification. The python wrapper StanfordCoreNLP (by Stanford NLP Group, only commercial license) and NLTK dependency grammars can be used to generate dependency trees.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## Tokenization\n\nThe process of splitting text into smaller pieces or units.  We want to tokenize text into sentences, and sentences into tokens.  The library provides a tokenization module, nltk.tokenize", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "import nltk\nfrom nltk import sent_tokenize, word_tokenize\nfrom IPython.display import Image\n\nnltk.download()", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "sentences = sent_tokenize(\"Our mission is to help people and communities achieve better health declares our purpose \" \\\n                          \"as a company and it serves as the standard against which we weigh our actions and our decisions. \" \\\n                          \"Our Vision is to be a leading innovator enabling healthy communities is both the inspirational and \" \\\n                          \"aspirational description of the future state of our company. It is our framework and guides every \" \\\n                          \"aspect of our business. By broadening our scope and continuing to evolve, we have more flexibility \" \\\n                          \"to make a greater impact on as many people as possible. Our core values are timeless. They \" \\\n                          \"the core principles that distinguish our culture and serve as a compass for our actions and describe \" \\\n                          \"how we behave in the world.\") \n\nsentences", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "tokens = word_tokenize(sentences[2])", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "tokens", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "#### Part of speech tagging \n\nApart from the grammar relations, every word in a sentence is also associated with a part of speech (pos) tag (nouns, verbs, adjectives, adverbs etc). The pos tags defines the usage and function of a word in the sentence. Here is a list of all possible pos-tags defined by Pennsylvania university. Following code using NLTK performs pos tagging annotation on input text. (it provides several implementations, the default one is perceptron tagger)", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from nltk import pos_tag \n#this is a Classifier, given a token assign a class\n#pos_tag Already defined in the library. We can train our own. ", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "tags = pos_tag(tokens)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "text = \"I am using Data Science Experience at Florida Blue for Natural Language Processing\"\ntokens = word_tokenize(text)\nprint pos_tag(tokens)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "# Let's apply this to our sample text from our website.\ntags", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "Part of Speech tagging is used for many important purposes in NLP:\n\n#### Word Sense Disambiguation\n\nSome language words have multiple meanings according to their usage. For example, in the two sentences below:\n\nI. \u201cPlease book my flight for Delhi\u201d\n\nII. \u201cI am going to read this book in the flight\u201d\n\n\u201cBook\u201d is used with different context, however the part of speech tag for both of the cases are different. In sentence I, the word \u201cbook\u201d is used as v erb, while in II it is used as no un. (Lesk Algorithm is also us ed for similar purposes)\n\n#### Improving word-based features\n\nA learning model could learn different contexts of a word when used word as the features, however if the part of speech tag is linked with them, the context is preserved, thus making strong features. For example:\n\nSentence -\u201cbook my flight, I will read this book\u201d\n\nTokens \u2013 (\u201cbook\u201d, 2), (\u201cmy\u201d, 1), (\u201cflight\u201d, 1), (\u201cI\u201d, 1), (\u201cwill\u201d, 1), (\u201cread\u201d, 1), (\u201cthis\u201d, 1)\n\nTokens with POS \u2013 (\u201cbook_VB\u201d, 1), (\u201cmy_PRP$\u201d, 1), (\u201cflight_NN\u201d, 1), (\u201cI_PRP\u201d, 1), (\u201cwill_MD\u201d, 1), (\u201cread_VB\u201d, 1), (\u201cthis_DT\u201d, 1), (\u201cbook_NN\u201d, 1)\n\n#### Normalization and Lemmatization\n\nPOS tags are the basis of lemmatization process for converting a word to its base form (lemma).\n\n#### Efficient Stopword Removal \n\nPOS tags are also useful in efficient removal of stopwords.\n\nFor example, there are some tags which always define the low frequency / less important words of a language. For example: (IN \u2013 \u201cwithin\u201d, \u201cupon\u201d, \u201cexcept\u201d), (CD \u2013 \u201cone\u201d,\u201dtwo\u201d, \u201chundred\u201d), (MD \u2013 \u201cmay\u201d, \u201cmust\u201d etc)\n\n ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "### Word Senses\n\nIn linguistics, a word sense is one of the meanings of a word.  Until now, we worked with tokens and POS. So, for instance in \"the man sit down on the bench near the river.\", the token [bench] could be bench as a constructed object by humans where people sit, or the natural side where the river meets the land.\n\n- WordNet: A semantic graph for words. NLTK provides a interface to the API \n", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "<img src=\"https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcSFZ2l8g_3316qek21ZEIkTS0WIYs8-lfTvXtO3YGWHEGpdDiMG\">", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Lets see some functions to handle meanings in tokens. Wordnet provides the concept of synsets, as syntactic units for tokens", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "from nltk.corpus import wordnet as wn #loading wordnet module\n\nwn.synsets('human')", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "wn.synsets('human')[0].definition ", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "wn.synsets('human')[1].definition", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "human = wn.synsets('Human',pos=wn.NOUN)[0]\nhuman", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "human.hypernyms()", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "human.hyponyms() ", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "bike = wn.synsets('bicycle')[0]\nbike", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "girl = wn.synsets('girl')[1]\ngirl", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "bike.wup_similarity(human)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "girl.wup_similarity(human)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "## Chunks\n\nChunking is the process of collecting patterns of Part of Speech together, representing some meaning.  Analysis of a sentence which identifies the constituents (noun groups - \"[The red tree] grows near the river\", verbs, verb groups, etc.)\n\nOur goal is detect into digital text, thing like \"where are different entities located,\" or \"which person is employed by what organization\". Its the way in which we extract structured data (entities and relations) from unstructured text.\n\n", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "<img width=580px height=150px src=\"http://www.nltk.org/images/chunk-treerep.png\" >", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from nltk import word_tokenize,pos_tag\nfrom nltk.chunk import RegexpParser\n\nchunker = RegexpParser(r'''\nNP:\n{<DT><NN.*><.*>*<NN.*>}\n}<VB.*>{\n''')\n", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "print tags\nprint chunker.parse(tags)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "## Entity Recognition - Chunking\n\nPart of chunking. We look for chunks of Part Of Speech.  The goal is to detect entities: Person, Location, Time, etc.  I will use here the library ne_chunker. But we can train our own models, or provide grammar rules as before.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from nltk.chunk import ne_chunk", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "sentence = \"Daryl A. is the head of the coworking place Commoncode Corp. from where many people work in Melbourne, Australia.\"\npos_tags = pos_tag(word_tokenize(sentence))\npos_tags", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "from IPython.display import display\ndisplay(pos_tags)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "### Sentiment Analyisis", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Is a phrase expresing a positive opinion? a negative opinion? how can we measure that? We will decompose sentences into their smaller units: tokens, and we will measure how probable they distribute on positive/negative sentences along the text. ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "<h2> Lets Classify Text! </h2>", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "#We will use movie reviews, already separated as positive and negative. Specially interest are those bigrams, \n#that are in positive or negative sentences (pair of words (word1, word2) that appear consecutives.)\nfrom nltk.corpus import movie_reviews", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "import nltk.classify.util\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import movie_reviews\n\n#This is the function that given a word, return a dict {word:True}. This will be our feature in the classifier. \ndef word_feats(words):\n    return dict([(word, True) for word in words])\n\n#neg_ids, pos_ids keep all the files for negative reviews, and positive reviews respectively.\nneg_ids = movie_reviews.fileids('neg')\npos_ids = movie_reviews.fileids('pos')\n\n#So, lets take the positive/negative words, create the feature for such word, and store it in a negative/positive features list.\nneg_feats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in neg_ids]\npos_feats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in pos_ids]\n\n#Separating 3/4 of this featured words for training, 1/4 for test. \nneg_len_train = len(neg_feats)*3/4\npos_len_train = len(pos_feats)*3/4\n\ntrain_feats = neg_feats[:neg_len_train] + pos_feats[:pos_len_train]\ntest_feats = neg_feats[neg_len_train:] + pos_feats[pos_len_train:]\n\n#training a NaiveBayes Classifier with our training featured words.\nclassifier = NaiveBayesClassifier.train(train_feats)\n\n#Lts check accuracy.\nprint 'accuracy: ', nltk.classify.util.accuracy(classifier, test_feats)\n\n#Lets see which words fit best in each class.\nclassifier.show_most_informative_features()\n", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "#SO WE TRAINED A CLASSIFIER FOR MOVIE REVIEWS. IT MEANS, FOR EVERY WORD THAT WE TRAINED, \n#IT NOWS THAT THIS WORD IS PROBABLE IN NEGATIVES WITH PROB P(W/POS) AND POSITIVE P(W/POS) (BAYES THEOREME). \n", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "sentence = \"Florida Blue, movie is incredible!\"\ntokens = [word for word in word_tokenize(sentence)]\n\npos_tags = [pos for pos in pos_tag(tokens)]\n\npos_tags", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "feats = word_feats( [word for (word,_) in pos_tags] )\nfeats", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "classifier.classify(feats)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "sentence = \"This is a miserable experience, and I just want to leave and be a lumberjack.\"\ntokens = [word for word in word_tokenize(sentence)]\n\npos_tags = [pos for pos in pos_tag(tokens) if pos[1] == 'JJ']\n\npos_tags", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "feats = word_feats( [word for (word,_) in pos_tags] )\nfeats", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "classifier.classify(feats)", 
            "cell_type": "code", 
            "execution_count": null
        }
    ], 
    "nbformat": 4
}