{
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "name": "python", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.2"
        }, 
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 2.0", 
            "name": "python3-spark20", 
            "language": "python"
        }
    }, 
    "nbformat_minor": 1, 
    "cells": [
        {
            "metadata": {}, 
            "source": "<table style=\"border: none\" align=\"left\">\n   <tr style=\"border: none\">\n      <th style=\"text-align: left;border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Train and deploy a heart disease prediction model using XGBoost and IBM Watson Machine Learning APIs</b></th>\n      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n   </tr>\n   <tr style=\"border: none\">\n       <th style=\"border: none\"><img src=\"https://github.com/pmservice/drug-selection/raw/master/images/heart_banner.png\" width=\"600\" alt=\"Icon\"> </th>\n   </tr>\n</table>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "This notebook demonstrates how to train a model using the XGBoost library to classify whether a person has heart disease or not. In addition to training, the notebook also explains how to persist a trained model to IBM Watson Machine Learning repository, deploy the model as a REST service and to predict using the deployed model using the REST APIs.  \n\nIn order to train and test the heart disease prediction model, you will be using an open source data set published in the University of California, Irvine (UCI) Machine Learning Repository. \n\nThis notebook uses Python 3.5 runtime, XGBoost 0.6 and Scikit-Learn 0.17.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## Learning goals\n\nThe learning goals of this notebook are:\n\n-  Load a CSV file into Pandas DataFrame.\n-  Prepare data for training and evaluation.\n-  Create DMatrix from a Pandas DataFrame.\n-  Create, train and evaluate a XGBoost model.\n-  Vizualize a decision trees used by the model.\n-  Vizualize the importance of features that were used to train the model.\n-  Persist a model in Watson Machine Learning repository using Python client library.\n-  Deploy a model for online scoring using the Watson Machine Learning's REST APIs\n-  Score a sample scoring data using the Watson Machine Learning's REST APIs.\n\n", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## Table of contents\nThis notebook contains the following sections:\n\n1.\t[Setup](#setup)\n2.\t[Load and explore data](#load)\n3.\t[Create XGBoost model](#model)\n4.\t[Persist model](#persistence)\n5.\t[Deploy and score in a Cloud](#scoring)\n6.\t[Summary and next steps](#summary)", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "<a id=\"setup\"></a>\n## 1. Setup\n\nBefore you execute the sample code in this notebook, you must perform the following setup tasks:\n\n-  Create a [Watson Machine Learning Service](https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/) instance (a free plan is offered). \n-  Download **Heart Disease Data Set** data in the Notebook's local filesystem\n-  Make sure that you are using a Spark 2.0 kernel.\n\n### Download Heart Disease Data Set  to Notebook's local filesystem\nHeart Disease Data Set is a freely available data set on the UCI Machine Learning Repository portal.\n\n1.  The **Heart Disease Data Set** is hosted [here](http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data).\n2.  In the following cell, replace the `link_to_data` variable value with the URL mentioned above.\n", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "In order to download the data from UCI Machine Learning Repository, use the `wget` library. Please install this library if have you have not installed it already. Use the following command to install the `wget` library: `!pip install wget --user` ", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "!pip install wget --user", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "Now, the code in the cell below downloads the data set and saves it in the local filesystem. The name of downloaded file containing the data will be displayed in the output of this cell.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "import wget\n\nlink_to_data = 'http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data'\nClevelandDataSet = wget.download(link_to_data)\n\nprint(ClevelandDataSet)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "The .csv file, **processed.cleveland.data**, that contains the heart disease data set is now availble on your local gpfs filesystem. ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "The downloaded data set contains the following attributes pertaining to heart disease.\n\n### Data set Details:\n1. age - age in years\n2. sex - sex(1 =  male; 0 = female)\n3. cp - chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 4 = asymptomatic)\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n5. chol - serum cholestoral in mg/dl\n6. fbs - fasting blood sugar > 120 mg/dl (1 = true; 0 = false)\n7. restecg - resting electrocardiographic results (0 = normal; 1 = having ST-T; 2 = hypertrophy)\n8. thalach - maximum heart rate achieved\n9. exang - exercise induced angina (1 = yes; 0 = no)\n10. oldpeak - ST depression induced by exercise relative to rest\n11. slope - the slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping)\n12. ca - number of major vessels (0-3) colored by flourosopy\n13. thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n14. num - the predicted attribute - diagnosis of heart disease (angiographic disease status) (Value 0 = < 50% diameter narrowing; Value 1 = > 50% diameter narrowing)\n", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "<a id=\"load\"></a>\n## 2. Load and explore data", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "In this section you will load the data as a Pandas data frame and perform a basic exploration.\n", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Load the data in the .csv file, **processed.cleveland.data**, into a Pandas data frame by running the following code:", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "import pandas as pd", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "col_names = ['age','sex','cp','restbp','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num']\n\nheart_data_df = pd.read_csv(ClevelandDataSet, sep=',', header=None, names=col_names, na_filter= True, na_values= {'ca': '?', 'thal': '?'})\nheart_data_df.head()", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "Let us see how many attributes and samples we have in this data set", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "(samples, attributes) = heart_data_df.shape\nprint(\"No. of Sample data =\", samples )\nprint(\"No. of Attributes  =\", attributes)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "We have 303 rows of sample data with 14 columns of data per sample.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "<a id=\"create\"></a>\n## 3. Create an XGBoost model", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "In recent years, ensemble learning models took the lead and became popular among machine learning practitioners.\n\nEnsemble learning model employs multiple machine learning algorithms to overcome the potential weaknesses of a single model. For example, if you are going to pick a destination for your next vacation, you probably ask your family and friends, read reviews and blog posts. Based on all the information you have gathered, you make your final decision.\n\nThis phenomenon is referred as the Wisdom of Crowds (WOC) in social sciences and it states that averaging the answers (prediction or probability) of a group will often result better than the answer of one of its members. The idea is that the collective knowledge of diverse and independent individuals will exceed the knowledge of any one of those individuals, helping to eliminate the noise.\n\nXGBoost is an open source library for ensemble based algorithms. It can be used for classification, regression and ranking type of problems. XGBoost supports multiple languages, such as C++, Python, R, and Java. \n\nThe Python library of XGBoost supports the following API interfaces to train and predict a model, also referred to as a `Booster`: \n- XGBoost's native APIs pertaining to the `xgboost` package, such as `xgboost.train()` or `xgboost.Booster`\n- Scikit-Learn based Wrapper APIs: `xgboost.sklearn.XGBClassifier` and `xgboost.sklearn.XGBRegressor`\n\nDetails about using the scikit-learn based Wrapper APIs to create and predict an XGBoost model is explained in the the [Classify tumors with machine learning](https://apsportal.ibm.com/exchange/public/entry/view/ac820b22cc976f5cf6487260f4c8d9c8) notebook.\n\nIn this section you will learn how to train and test an XGBoost model using XGBoost's native python APIs. ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "First, you must import the required libraries.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "import xgboost as xgb\n\nimport pandas as pd\n\nfrom sklearn import cross_validation\nfrom sklearn.metrics import accuracy_score\n\nfrom matplotlib import pyplot\nimport pprint\n%matplotlib inline", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "### 3.1: Prepare data", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "In this section, clean and transform the data in the Pandas data frame into the data that can be given as input for training the model. \n", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "#### 3.1.1: Cleanse the data\nFirst, check if there are any null data in our dataset and remove the corresponding rows.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "print(\"List of features with their corresponding count of null values : \")\nprint(\"---------------------------------------------------------------- \")\nprint(heart_data_df.isnull().sum())", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "From the output of the above cell, there are 6 occurrences where there are null values. The rows containing these null values can be removed so that the data set does not have any incomplete data. The cell below contains the command to remove the rows that contain these null values.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "heart_data_df = heart_data_df.dropna(how='any',axis=0)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "#### 3.1.2: Prepare the target data and feature columns", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "In this section, transform the existing data frame to derive the target data that contains the prediction value for the corresponding sample data. \n\nThe goal of the model here is to predict whether a patient has a heart problem. Although the data set currently available does not have this information, this information can be derived from the `num` attribute. The `num` column and its values pertain to the number of major vessels with more than 50% narrowing (values- 0,1,2,3 or 4) for the corresponding sample data. \n\nTherefore, the target column `diagnosed` can derived in the following way: \n- 'diagnosed' is '0' when 'num' = 0 , indicating normal heart functioning \n- 'diagnosed' is '1' when 'num' > 0 , indicating a heart problem.\n", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "\nheart_data_df['diagnosed'] = heart_data_df['num'].map(lambda d: 1 if d > 0 else 0)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "The next step is to select the attributes in the current data set that can be used for training the model. Here, all the attributes other than `num` attribute are chosen as the features.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "feature_cols = ['age','sex','cp','restbp','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal']\nfeatures_df = heart_data_df[feature_cols]", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "#### 3.1.3: Split the data set for training and testing", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "As the target and feature columns has been defined, you can now split the data set into two sets that will be used for training the model and for testing the trained model. ", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "heart_train, heart_test, target_train, target_test = cross_validation.train_test_split(features_df, heart_data_df.loc[:,'diagnosed'], test_size=0.33, random_state=0)\n", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "#### 3.1.4: Construct DMatix objects ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "DMatrix is the data interface provided by the XGBoost library. The training data and test data are converted as DMatrix objects to perform training and to make predictions. The DMatrix objects can be created from various data formats, such as Numpy arrays, Pandas data frames, or a Scipy sparse array. For more information about the DMatrix interface, see [Python Package Introduction](http://xgboost.readthedocs.io/en/latest/python/python_intro.html).", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Next, prepare the **DMatrix** objects for training and testing based on the training and test data that was split above", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "dm_train = xgb.DMatrix(heart_train, label=target_train)\ndm_test = xgb.DMatrix(heart_test)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "### 3.2: Create XGBoost model (Booster)", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Set the parameters of the Booster that we are about to create and train.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "param = {'objective':'multi:softmax', 'max_depth':2, 'eta':0.8, 'num_class': 2, 'eval_metric': 'auc', 'silent':1 }", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "Create a Booster by using the training data set, which is in the form of a DMatrix object.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "xgb_model = xgb.train(param, dm_train)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "Make predictions on test data and evaluate the model.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "y_predict = xgb_model.predict(dm_test)\nprint(y_predict)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "Evaluate the performance of the model using the predicted data.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "accuracy = accuracy_score(target_test, y_predict)\nprint(\"Accuracy: \" +  str(accuracy))", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "To understand the model better, XGBoost provides APIs that you can use to get insights about the trees used for training the model and the importance of the features in constructing the Booster.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "To plot graphs you must use the commands in the following cell to set up the notebook for plotting the graphs.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "import matplotlib.pyplot as plt\n%matplotlib inline", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "The following cell contains the command to plot the graph depicting the importance of features.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "xgb.plot_importance(xgb_model)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "To visualize the decision trees that are trained by XGBoost, that is, the XGBoost model you must first install the `graphviz` package. The `graphviz` Python package is installed in the notebook environment by running the following cell: ", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "!pip install graphviz", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "xgb.plot_tree(xgb_model, num_trees=1)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "<a id=\"persistence\"></a>\n## 4. Persist the model", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "In this section store the XGBoost model in the Watson Machine Learning repository by using Watson Machine Learning repository service Python client libraries.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from repository.mlrepository import MetaNames\nfrom repository.mlrepository import MetaProps\nfrom repository.mlrepositoryclient import MLRepositoryClient\nfrom repository.mlrepositoryartifact import MLRepositoryArtifact", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "Authenticate to Watson Machine Learning service on Bluemix. \n\n\n\n**Action**: Put authentication information from your instance of Watson Machine Learning service in the following cell. \n<br>\n<br>\n**Tip**: service_path, user and password can be found on **Service Credentials** tab of service instance created in Bluemix. If you cannot see **instance_id** field in **Serice Credentials** generate new credentials by pressing **New credential (+)** button. \n", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "\n# @hidden_cell\nwml_credentials = {\n  \"url\": \"https://ibm-watson-ml.mybluemix.net\",\n  \"access_key\": \"$ACCESSKEY\",\n  \"username\": \"$USERNAME\",\n  \"password\": \"$PASSWORD\",\n  \"instance_id\": \"$INSTANCEID\"\n}\n", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "ml_repository_client = MLRepositoryClient(wml_credentials['url'])\nml_repository_client.authorize(wml_credentials['username'], wml_credentials['password'])", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "### 4.1: Save a XGBoost model in the Machine Learning Repository\n\nIn this subsection you will learn how to save a model artifact to your Watson Machine Learning instance by using the Watson Machine Learing repository Python client package.\n\nCreate an artifact and save it to the machine learning repository by running the following cells:", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Check if props is mandatory\nprops1 = MetaProps({MetaNames.AUTHOR_NAME:\"YOUR_name\", MetaNames.AUTHOR_EMAIL:\"Your_email@email.com\"})\n\nmodel_artifact = MLRepositoryArtifact(xgb_model, name='XGB_Heart_Disease_Detection', meta_props=props1)\n\nsaved_model = ml_repository_client.models.save(model_artifact)\n", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "saved_model_meta = saved_model.meta.get()\npprint.pprint(saved_model_meta)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "#### 4.2 Load the Booster from the saved model", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "saved_model_meta['modelVersionHref']", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "loaded_artifact = ml_repository_client.models.version_from_href(saved_model_meta['modelVersionHref'])\nloaded_xgb_model = loaded_artifact.model_instance()\nprint(\"Type of model: \" + str(type(loaded_xgb_model)))", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "y_lpredict = loaded_xgb_model.predict(dm_test)\nprint(y_lpredict)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "#### 4.3 Load the Booster as a XGBRegressor\nWatson Machine Learning Repository's Python client provides an option to load the XGBoost model i.e Booster as a Scikit-Learn wrapper - XGBRegressor. The command in cell below is used to for this purpose", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "loaded_artifact = ml_repository_client.models.version_from_href(saved_model_meta['modelVersionHref'])\nloaded_xgb_regressor = loaded_artifact.model_instance(as_type=\"XGBRegressor\")", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "print(\"Type of model: \" + str(type(loaded_xgb_regressor)))", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "y_pred_xgb_reg = loaded_xgb_regressor.predict(heart_test)\nprint(y_pred_xgb_reg)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "<a id=\"scoring\"></a>\n## 5. Deploy and score in a Cloud", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "To access the Watson Machine Learning REST APIs we require a Watson Machine Learning access token. \n\nTo create the WML access token, run the commands in the following cell:", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "import urllib3, requests, json\n\nheaders = urllib3.util.make_headers(basic_auth='{}:{}'.format(wml_credentials['username'], wml_credentials['password']))\nurl = '{}/v3/identity/token'.format(wml_credentials['url'])\nresponse = requests.get(url, headers=headers)\nmltoken = json.loads(response.text).get('token')\nheader = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken}", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "### 5.1: Deploy the model and create online scoring endpoint", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Get the `published_models` URL from instance details.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "endpoint_instance = wml_credentials['url'] + \"/v3/wml_instances/\" + wml_credentials['instance_id']\nheader = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken} \n\nresponse_get_instance = requests.get(endpoint_instance, headers=header)\nprint(response_get_instance)\nprint(response_get_instance.text)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {
                "scrolled": false
            }, 
            "source": "endpoint_published_models = json.loads(response_get_instance.text).get('entity').get('published_models').get('url')\nprint(endpoint_published_models)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "Execute the following sample code that uses the `published_models` endpoint to get the deployments URL.\n\nGet the list of published models.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken}\n\nresponse_get = requests.get(endpoint_published_models, headers=header)\nprint(response_get)\nprint(response_get.text)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "Get published model deployment URL.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "[endpoint_deployments] = [x.get('entity').get('deployments').get('url') for x in json.loads(response_get.text).get('resources') if x.get('metadata').get('guid') == saved_model.uid]\n\nprint(endpoint_deployments)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "#### 5.1.2 Deploy Model", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "We can now create the online deployment for the published model.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "payload_online = {\"name\": \"xgb_heart_disease_v1\", \"description\": \"xgb_heart_disease\", \"type\": \"online\"}\nresponse_online = requests.post(endpoint_deployments, json=payload_online, headers=header)\n\nprint(response_online.text)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "scoring_url = json.loads(response_online.text).get('entity').get('scoring_url')\nprint(scoring_url)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "### 5. 2 Perform Prediction", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Now, let us perform predictions on a new set of data using the model that is deployed in the scoring service.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "source": "payload_scoring = {\n   \"values\": [[64.0, 1.0, 4.0, 328.0, 263.0, 0.0, 0.0, 105.0, 1.0, 0.2, 2.0, 1.0, 7.0]]\n}\n\nresponse_scoring = requests.post(scoring_url, json=payload_scoring, headers=header)\npprint.pprint(response_scoring.text)", 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "metadata": {}, 
            "source": "The scoring output contains the prediction value and the corresponding margin data.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "<a id=\"summary\"></a>\n## 6. Summary and next steps     ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "You successfully completed this notebook! You learned how to use XGBoost machine learning as well as Watson Machine Learning for model creation and deployment. Check out our [Online Documentation](https://console.ng.bluemix.net/docs/services/PredictiveModeling/pm_service_api_spark.html) for more samples, tutorials, documentation, how-tos, and blog posts. ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "### Author\n\n**Krishnamurthy Arthanarisamy**, is a senior technical lead in IBM Watson Machine Learning team. Krishna works on developing cloud services that caters to different stages of machine learning and deep learning modeling life cycle. ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Copyright \u00a9 2017 IBM. This notebook and its source code are released under the terms of the MIT License.", 
            "cell_type": "markdown"
        }
    ], 
    "nbformat": 4
}