{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "# IBM Speech to Text\n\nThe IBM\u00ae Speech to Text service provides an Application Programming Interface (API) that lets you add speech transcription capabilities to your applications. To transcribe the human voice accurately, the service leverages machine intelligence to combine information about grammar and language structure with knowledge of the composition of the audio signal. The service continuously returns and retroactively updates the transcription as more speech is heard.\n\nThe service provides a variety of interfaces to suit the needs of your application. It supports many features that make it suitable for numerous use-cases. And it provides a customization interface that lets you enhance its base language and acoustic capabilities with vocabularies and acoustic characteristics specific to your domain, environment, and speakers.\nSupported interfaces\n\nThe Speech to Text service offers four interfaces:\n\n - A WebSocket interface for establishing persistent, full-duplex connections with the service for speech transcription.\n - An HTTP REST interface that supports both sessionless and session-based calls to the service for speech recognition.\n - An asynchronous HTTP interface that provides non-blocking calls to the service for speech recognition.\n - A customization interface that lets you expand the vocabulary of a base model with domain-specific terminology or adapt a base model for the acoustic characteristics of your audio.\n\nSDKs are also available that simplify using the service's interfaces in various programming languages. For more information about application development with the service, see [Overview for developers](https://console.bluemix.net/docs/services/speech-to-text/developer-overview.html).", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Speech to Text Parameters\n\n### acoustic_customization_id\n\nAn optional customization ID for a custom acoustic model that is adapted for the acoustic characteristics of your environment and speakers. By default, no custom model is used. See Custom models.\n\n### customization_id\n\nAn optional customization ID for a custom language model that includes terminology from your domain. By default, no custom model is used. See Custom models.\n\n### customization_weight\n\nAn optional double between 0.0 and 1.0 that indicates the relative weight that the service gives to words from a custom language model compared to those from the base vocabulary. The default is 0.3 unless a different weight was specified when the custom language model was trained. See Custom models.\n\n### inactivity_timeout\n\nAn optional integer that specifies the number of seconds for the service's inactivity timeout; use -1 to indicate infinity. The default is 30 seconds. See Inactivity timeout.\n\n### interim_results\n\nAn optional boolean that directs the service to return intermediate hypotheses that are likely to change before the final transcript. By default (false), interim results are not returned. See Interim results.\n\n### keywords\n\nAn optional array of keyword strings that the service spots in the input audio. By default, keyword spotting is not performed. See Keyword spotting.\n\n### keywords_threshold\n\nAn optional double between 0.0 and 1.0 that indicates the minimum threshold for a positive keyword match. By default, keyword spotting is not performed. See Keyword spotting.\n\n### max_alternatives\n\nAn optional integer that specifies the maximum number of alternative hypotheses that the service returns. By default, the service returns a single final hypothesis. See Maximum alternatives.\n\n### model\n\nAn optional model that specifies the language in which the audio is spoken and the rate at which it was sampled, broadband or narrowband. By default, the en-US_BroadbandModel model is used. See Language and models.\n\n### profanity_filter\n\nAn optional boolean that indicates whether the service censors profanity from a transcript. By default (true), profanity is filtered from the transcript. See Profanity filtering.\n\n### smart_formatting\n\nAn optional boolean that indicates whether the service converts dates, times, numbers, currency, and similar values into more conventional representations in the final transcript. By default (false), smart formatting is not performed. See Smart formatting.\n\n### speaker_labels\n\nAn optional boolean that indicates whether the service identifies which individuals spoke which words in a multi-participant exchange. By default (false), speaker labels are not returned. See Speaker labels.\n\n### timestamps\n\nAn optional boolean that indicates whether the service produces timestamps for the words of the transcript. By default (false), timestamps are not returned. See Word timestamps.\n\n### Transfer-Encoding\n\nAn optional value of chunked that causes the audio to be streamed to the service. By default, audio is sent all at once as a one-shot delivery. See Audio transmission.\n\n### watson-token\n\nAn optional authentication token that makes authenticated requests to the service without embedding your service credentials in every call. By default, service credentials must be passed with each request. See Authentication tokens and request logging.\n\n### word_alternatives_threshold\n\nAn optional double between 0.0 and 1.0 that specifies the threshold at which the service reports acoustically similar alternatives for words of the input audio. By default, word alternatives are not returned. See Word alternatives.\n\n### word_confidence\n\nAn optional boolean that indicates whether the service provides confidence measures for the words of the transcript. By default (false), word confidence measures are not returned. See Word confidence.\n\n### X-Watson-Authorization-Token\n\nAn optional authentication token that makes authenticated requests to the service without embedding your service credentials in every call. By default, service credentials must be passed with each request. See Authentication tokens and request logging.\n\n### X-Watson-Learning-Opt-Out\n\nAn optional boolean that indicates whether you opt out of the request logging that IBM performs to improve the service for future users. By default (false), request logging is performed. See Authentication tokens and request logging.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Import Libraries\n\nBring in a few libraries we need and extend DSX with some libraries not currently installed.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "!pip install --upgrade watson_developer_cloud\n\nimport requests\nimport json\nimport os\n\nfrom os.path import join, dirname\nfrom watson_developer_cloud import SpeechToTextV1", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Authentication Handling and File Details\n\nParameters for our Authentication Credentials for our Speech to Text API and the Input Files we'll be leveraging.\n\n### ATTENTION:  SET SPEECH SERVICE CREDENTIALS", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "source": "\n# @hidden_cell\nurl = \"https://stream.watsonplatform.net/speech-to-text/api/v1/recognize\"\nusername= \"$USERNAME\" \npassword= \"$PASSWORD\"\n\nfile1 = \"https://github.com/krondor/nlp-dsx-pot/raw/master/aging.mp3\"\nfile2 = \"http://podcast.c-span.org/podcast/SBHAR1020.mp3\"\nfile3 = \"https://github.com/krondor/nlp-dsx-pot/raw/master/reagan-thatcher.mp3\"", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Basic transcription with CURL\n\nBy default, the service returns a basic transcription of the input audio. The following example cURL command submits a brief FLAC file named audio-file.flac External link icon with no additional output parameters, and the service returns basic transcription results.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "!wget {file1} -O aging.mp3 -nc\n\n# Define Local File for CURL\nfilepath = './reagan-thatcher.mp3'\n\n!curl -X POST -u {username}:{password} \\\n    --header \"Content-Type: audio/mp3\" \\\n    --data-binary @{filepath} \\\n    \"https://stream.watsonplatform.net/speech-to-text/api/v1/recognize\"", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Output Handling with Requests\n\nBy default, the service returns a basic transcription of the input audio. In this example we will use the requests library to structure our post and pass parameters to the service.  We will define diarization requirement and select our speaking model.  The results will be passed to a pandas data frame and subsequently analyzed.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "!wget {file3} -O reagan-thatcher.mp3 -nc \n\nfilename = os.path.basename(filepath)\n\naudio = open(filename,'rb')\n\nfiles_input = {\n    \"audioFile\":(filename,audio,'audio/mp3')    \n}\n\n# Define Speech to Text Feature Parameters\nparams = (\n    ('model', 'en-US_NarrowbandModel'),\n    ('speaker_labels', 'true')\n)\n\nresponse = requests.post(\n    url, \n    params=params,\n    auth=(username, password), \n    headers={\"Content-Type\": \"audio/mp3\"},\n    files=files_input)\n\nresponse_data = response.json()\nprint('status_code: {} (reason: {})'.format(response.status_code, response.reason))", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Pandas from Results", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "import pandas as pd\n\ndata = []\n\nfor item in response_data['results']:\n    for trans in item['alternatives']:\n        data.append(dict({'transcript':trans['transcript'], 'confidence':trans['confidence']}))\n\n# Create Pandas Data Frame of Transcript Results with Confidence\ndf = pd.DataFrame(data)\n\n# View Snippet\ndf.head(5)", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Confidence Spread\n\nLet's plot the confidence per transcription snippet to get a rough idea of the data integrity in our transcription.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "%matplotlib inline\n\nimport numpy as np\nimport matplotlib\n\nmatplotlib.style.use('ggplot')\n\nplt.figure();\n\ndf['confidence'].plot.hist()", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Speech to Text with Watson Developer Cloud SDK", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "source": "speech_to_text = SpeechToTextV1(\n    username='d9f7864a-0869-40ee-98af-58e23e996a0e',\n    password='nYmlwq7VBTZz',\n    x_watson_learning_opt_out=False\n)\n\n!wget {file1} -O aging.mp3 -nc\n\nfilepath = './aging.mp3'  # path to file\nfilename = os.path.basename(filepath)\n\nprint(json.dumps(speech_to_text.models(), indent=2))\n\nprint(json.dumps(speech_to_text.get_model('en-US_BroadbandModel'), indent=2))\n\nwith open(filename, 'rb') as audio_file:\n    print(json.dumps(speech_to_text.recognize(\n        audio_file, content_type='audio/mp3', timestamps=True,\n        word_confidence=True, speaker_labels=True),\n        indent=2))\n", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Extra Credit\n\nThe websockets API provides for streaming interface to speech to text translation. \n\n[Speech to Text Websockets](https://github.com/watson-developer-cloud/speech-to-text-websockets-python)", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "nbformat_minor": 1, 
    "metadata": {
        "kernelspec": {
            "language": "python", 
            "name": "python3-spark21", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.1"
        }, 
        "language_info": {
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "file_extension": ".py", 
            "version": "3.5.2", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "name": "python"
        }
    }, 
    "nbformat": 4
}