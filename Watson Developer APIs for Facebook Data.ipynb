{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "# Analyze Facebook Data Using IBM Watson and IBM Data Platform\n\nThis is a three-part notebook written in `Python_3.5` meant to show how anyone can enrich and analyze a combined dataset of unstructured and strucutured information with IBM Watson and IBM Data Platform. For this example we are using a standard Facebook Analytics export which features texts from posts, articles and thumbnails, along with standard performance metrics such as likes, shares, and impressions. \n\n**Part I** will use the Natual Language Understanding, Visual Recognition and Tone Analyzer Services from IBM Watson to enrich the Facebook Posts, Thumbnails, and Articles by pulling out `Emotion Tones`, `Social Tones`, `Language Tones`, `Entities`, `Keywords`, and `Document Sentiment`. The end result of Part I will be additional features and metrics we can test, analyze, and visualize in Part III. \n\n**Part II** will be used to set up the visualizations and tests we will run in Part III. The end result of Part II will be multiple Pandas DataFrames that will contain the values, and metrics needed to find insights from the Part III tests and experiments.\n\n**Part III** will include services from IBM's Data Platform, including IBM's own data visualization library PixieDust. In Part III we will run analysis on the data from the Facebook Analytics export, such as the number of likes, comments, shares, to the overall reach for each post, and will compare it to the enriched data we pulled in Part I.\n\n\n#### You should only need to change data in the Setup portion of this notebook. All places where you see  <span style=\"color: red\"> User Input </span> is where you should be adding inputs. \n\n### Table of Contents\n\n### [**Part I - Enrich**](#part1)<br>\n1. [Setup](#setup)<br>\n   1.1 [Install Watson Developer Cloud and BeautifulSoup Packages](#setup1)<br>\n   1.2 [Install PixieDust](#pixie)<br> \n   1.3 [Import Packages and Libraries](#setup2)<br>\n   1.4 [Add Service Credentials From Bluemix for Watson Services](#setup3)<br>\n2. [Load Data](#load)<br>\n   2.1 [Load Data From SoftLayer's Object Storage as a Pandas DataFrame](#load1)<br>\n3. [Prepare Data](#prepare)<br>\n   3.1 [Data Cleansing with Python](#prepare1)<br>\n   3.2 [Beautiful Soup to Extract Thumbnails and Extented Links](#prepare2)<br>\n4. [Enrich Data](#enrich)<br>\n   4.1 [NLU for Post Text](#nlupost)<br>\n   4.2 [NLU for Thumbnail Text](#nlutn)<br>\n   4.3 [NLU for Article Text](#nlulink)<br>\n   4.4 [Tone Analyzer for Post Text](#tonepost)<br>\n   4.5 [Tone Analyzer for Article Text](#tonearticle)<br>\n   4.6 [Visual Recognition](#visual)<br>\n5. [Write Data](#write)<br>\n   5.1 [Convert DataFrame to new CSV](#write1)<br>\n   5.2 [Write Data to SoftLayer's Object Storage](#write2)<br>\n    \n### [**Part II - Data Preperation**](#part2)<br>\n1. [Prepare Data](#prepare)<br>\n   1.1 [Create Multiple DataFrames for Visualizations](#visualizations)<br>\n   1.2 [Create A Consolidated Tone Dataframe](#tone)<br>\n   1.3 [Create A Consolidated Keyword Dataframe](#keyword)<br>\n   1.4 [Create A Consolidated Entity Dataframe](#entity)<br>\n  \n### [**Part III - Analyze**](#part3)<br>\n\n1. [Setup](#2setup)<br> \n    1.1 [Assign Variables](#2setup2)<br>\n2. [Load Data](#2load)<br>\n    2.1 [Load Data From SoftLayer's Object Storage as a Spark SQL DataFrame](#2load1)<br>\n3. [Visualize Data](#2visual)<br>\n    3.1 [Run PixieDust Visualization Library with Display() API](#2visual2)\n   \n### Learn more about the technology used:\n\n* [Natual Language Understanding](https://www.ibm.com/watson/developercloud/natural-language-understanding.html)\n* [Tone Analyzer](https://www.ibm.com/watson/developercloud/tone-analyzer.html)\n* [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n* [PixieDust](https://github.com/ibm-cds-labs/pixiedust) (Part III)", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Sample Documents\n1. [Sample Facebook Posts](https://ibm.box.com/s/25trtr37krt34s9swpxa1jb7fkd63ntm) - This is a sample export of IBM Watson's Facebook Page. Engagement metrics such as clicks, impressions, etc. are all changed and do not reflect any actual post performance data.\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"part1\"></a>\n#  Part I - Enrich\n\n\n## 1. Setup\n<a id=\"setup1\"></a>\n### 1.1 Install Latest Watson Developer Cloud and Beautiful Soup Packages", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "!pip install --upgrade watson-developer-cloud", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "!pip install --upgrade beautifulsoup4", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "If WDC or BS4 was just installed or upgraded, <span style=\"color: red\">restart the kernel</span> before continuing", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"pixie\"></a>\n### 1.2 Install PixieDust Library\nThis notebook provides an overview of how to use the PixieDust Library to analyze and visualize various data sets. If you are new to PixieDust or would like to learn more about the library, please go to this [Introductory Notebook](https://apsportal.ibm.com/exchange/public/entry/view/5b000ed5abda694232eb5be84c3dd7c1) or visit the [PixieDust Github](https://ibm-cds-labs.github.io/pixiedust/). The `Setup` section for this notebook uses instructions from the [Intro To PixieDust](https://github.com/ibm-cds-labs/pixiedust/blob/master/notebook/Intro%20to%20PixieDust.ipynb) notebook", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "To ensure you are running the latest version of PixieDust uncomment and run the following cell. Do not run this cell if you installed PixieDust locally from source and want to continue to run PixieDust from source.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "!pip install --user --upgrade pixiedust", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"setup2\"></a>\n### 1.3 Import Packages and Libraries\nTo check if you have package already installed, open new cell and write: *help.('Package Name')*", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "import json\nimport sys\nimport watson_developer_cloud\nfrom watson_developer_cloud import ToneAnalyzerV3, VisualRecognitionV3\nimport watson_developer_cloud.natural_language_understanding.features.v1 as features\n\nimport operator\nfrom functools import reduce\nfrom io import StringIO\nimport numpy as np\nfrom bs4 import BeautifulSoup as bs\nfrom operator import itemgetter\nfrom os.path import join, dirname\nimport pandas as pd\nimport numpy as np\nimport requests\nimport pixiedust", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id='setup3'></a>\n### 1.4 Add Service Credentials From Bluemix for Watson Services\n\nTo create your own service and API keys for either NLU or Tone Analyzer go to the Watson Services on [Bluemix](https://www.ibm.com/cloud-computing/bluemix/).\n\nAfter creating a service for NLU and Tone Analyzer, replace the credentials in the section below\n\n###  <span style=\"color: red\"> _User Input_</span> ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "nlu = watson_developer_cloud.NaturalLanguageUnderstandingV1(version='2017-02-27',\n                                                            username='9431253f-036c-4ee6-a74b-7615d4c7960f',\n                                                            password='LGVYmMvGBCpj')\ntone_analyzer = ToneAnalyzerV3(version='2016-05-19',\n                               username='201ae06f-a97c-4e04-9f6b-61bb1433bdd6',\n                               password='bj2OrmHz8yRc')\n\nvisual_recognition = VisualRecognitionV3('2016-05-20', api_key='610f09828874294ebd29fc834d3e1db6014c7c3f')\n", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id='load'></a> \n## 2. Load Data\n\n### 2.1 Load data from Object Storage\nIBM\u00ae Object Storage for Bluemix\u00ae provides provides you with access to a fully provisioned Swift Object Storage account to manage your data. Object Storage uses OpenStack Identity (Keystone) for authentication and can be accessed directly by using [OpenStack Object Storage (Swift) API v3](http://developer.openstack.org/api-ref-identity-v3.html#credentials-v3). \n\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "###  <span style=\"color: red\"> _User Input_</span> \n\nInsert data you want to enrich by clicking on the 1001 icon on the upper right hand of the screen. Click \"Insert to code\" under the file you want to enrich. The make sure you've clicked the cell below and then choose \"Insert Pandas DataFrame.\"", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# insert pandas dataframe\nfrom io import StringIO\nimport requests\nimport json\nimport pandas as pd\n\n# @hidden_cell\n# This function accesses a file in your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef get_object_storage_file_with_credentials_2ba2092e29ef46aa9396bf877110eac0(container, filename):\n    \"\"\"This functions returns a StringIO object containing\n    the file content from Bluemix Object Storage.\"\"\"\n\n    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n            'password': {'user': {'name': 'member_319c8cff568c05ab6aa5a99c320b49b1898b7794','domain': {'id': '2d34dd15924a4090902d72e8669bbef8'},\n            'password': 'U-ensM3S^*Q&x(9T'}}}}}\n    headers1 = {'Content-Type': 'application/json'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()\n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                        if(e2['interface']=='public'and e2['region']=='dallas'):\n                            url2 = ''.join([e2['url'],'/', container, '/', filename])\n    s_subject_token = resp1.headers['x-subject-token']\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.get(url=url2, headers=headers2)\n    return StringIO(resp2.text)\n\ndf_data_2 = pd.read_csv(get_object_storage_file_with_credentials_2ba2092e29ef46aa9396bf877110eac0('FloridaBlueNLPPOT', '2015 IBM Watson Facebook Export.csv'))\ndf_data_2.head()\n", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "###  <span style=\"color: red\"> _User Input_</span> ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#Make sure this equals the dataframe variable above. Usually it is df_data_1, but it can change\ndf = df_data_2", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "###  <span style=\"color: red\"> _User Input_</span> ", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "Put in the credentials for the file you want to enrich by clicking on the 1001 icon on the upper right hand of the screen. Click the cell below, then click \"Insert to code\" under the file you want to enrich. Choose \"Insert Credentials.\" **CHANGE THE NAME TO `credentials_1`**", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#insert credentials for file - Change to credentials_1\n# @hidden_cell\ncredentials_1 = {\n  'auth_url':'https://identity.open.softlayer.com',\n  'project':'object_storage_2ba2092e_29ef_46aa_9396_bf877110eac0',\n  'project_id':'245a1808376a4f4e9d32074e623ad8b6',\n  'region':'dallas',\n  'user_id':'4746cb39a63b47be9472ddc96d4f5447',\n  'domain_id':'2d34dd15924a4090902d72e8669bbef8',\n  'domain_name':'1037481',\n  'username':'member_319c8cff568c05ab6aa5a99c320b49b1898b7794',\n  'password':\"\"\"U-ensM3S^*Q&x(9T\"\"\",\n  'container':'FloridaBlueNLPPOT',\n  'tenantId':'undefined',\n  'filename':'2015 IBM Watson Facebook Export.csv'\n}\n", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "###  <span style=\"color: red\"> _User Input_</span> ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#choose any name to save your file\nlocalfilename = 'OutputFile.csv'", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id='prepare'></a>\n## 3. Prepare Data\n\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id='prepare1'></a>\n###  3.1 Data Cleansing with Python\nRenaming columns, removing noticable noise in the data, pulling out URLs and appending to a new column to run through NLU", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "df.rename(columns={'Post Message': 'Text'}, inplace=True)", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "df = df.drop([0])\n#df.head()", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "df_http= df[\"Text\"].str.partition(\"http\")\ndf_www = df[\"Text\"].str.partition(\"www\")\n\n#combine delimiters with actual links\ndf_http[\"Link\"] = df_http[1].map(str) + df_http[2]\ndf_www[\"Link1\"] = df_www[1].map(str) + df_www[2]\n\n#include only Link columns \ndf_http.drop(df_http.columns[0:3], axis=1, inplace = True)\ndf_www.drop(df_www.columns[0:3], axis=1, inplace = True)\n\n#merge http and www dataframes\ndfmerge = pd.concat([df_http, df_www], axis=1)\n\n#the following steps will allow you to merge data columns from the left to the right\ndfmerge = dfmerge.apply(lambda x: x.str.strip()).replace('', np.nan)\n\n#use fillna to fill any blanks with the Link1 column\ndfmerge[\"Link\"].fillna(dfmerge[\"Link1\"], inplace = True)\n\n#delete Link1 (www column)\ndfmerge.drop(\"Link1\", axis=1, inplace = True)\n\n#combine Link data frame \ndf = pd.concat([dfmerge,df], axis = 1)\n\n# # make sure text column is a string\ndf[\"Text\"] = df[\"Text\"].astype(\"str\")\n\n# #strip links from Text column\ndf['Text'] = df['Text'].apply(lambda x: x.split('http')[0])\ndf['Text'] = df['Text'].apply(lambda x: x.split('www')[0])\n\n#df.head()", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "#Pull thumbnail descriptions using beautiful soup\n#changes links from objects to strings\nfor link in df.Link:\n    df.Link.to_string()\n    \n#create empty list to store descriptions    \ndescription = []\n\n#use BeautifulSoup to pull descriptions from links \nfor url in df[\"Link\"]:\n    try:\n        #if there's no description\n        if pd.isnull(url):\n            description.append(\"\")\n        else:\n            page3= requests.get(url)\n            soup3= bs(page3.text,\"lxml\")\n            #Capture both capatalized 'Description' and lower case\n            desc= soup3.find(attrs={'name':'Description'})\n            if desc == None:\n                desc= soup3.find(attrs={'name':'description'})\n            description.append(desc['content'])\n            \n    #this exception will save you from 404 errors\n    except Exception:\n        description.append(\"\")\n        continue\n        \n#save to df and add column titled 'Thumbnails'\ndf[\"Thumbnails\"] = description\n#df['Thumbnails'].head()\n#df.head()", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "piclinks = []\n\nfor url in df[\"Link\"]:\n    try:\n        if pd.isnull(url):\n            piclinks.append(\"\")\n        else: \n            page3= requests.get(url)\n            soup3= bs(page3.text,\"lxml\")\n            pic = soup3.find('meta', property =\"og:image\")\n            if pic:\n                piclinks.append(pic[\"content\"])\n            else: \n                piclinks.append(\"\")\n    except:\n        piclinks.append(\"\")\ndf[\"Image\"] = piclinks ", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "**<span style=\"color: red\">Optional<span>**: Convert shortened links to full links (Note: NLU requires full links)\nUse requests module to pull extended lists. This is only necessary if the Facebook page uses different links than the articles themselves. For this example we are using IBM Watson's Facebook export which uses an IBM link. \n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#converts shortened links to their original form\n\nshortlink = df[\"Link\"]\nextendedlink = []\n\nfor link in shortlink:\n    #create empty list to store   \n    try:\n        extended_link = requests.Session().head(link, allow_redirects=True).url\n        extendedlink.append(extended_link)\n    except:\n         # catch *all* exceptions\n        e = sys.exc_info()[0]\n        extendedlink.append('')\n        pass\ndf[\"Extended Links\"] = extendedlink", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id='enrich'></a> \n## 4. Enrichment Time!\n<a id='nlupost'></a>\n###  4.1 NLU for the Post Text\nBelow uses Natural Language Understanding to iterate through each post and extract the enrichment features we want to use in our future analysis.\n\nEach feature we extract will be appended to the `.csv` in a new column we determine at the end of this script. If you want to run this same script for the other columns, define `free_form_responses` to the column name, if you are using URLs, change `text=response` parameter to `url=response`, and update the new column names as you see fit. ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Extract the free form text response from the data frame\n# If you are using this script for a diff CSV, you will have to change this column name\nfree_form_responses = df['Text']\n# define the list of enrichments to apply\n# if you are modifying this script add or remove the enrichments as needed\nf = [features.Entities(), features.Keywords(),features.Emotion(),features.Sentiment()]#'typed-rels'\n\n# Create a list to store the enriched data\noverallSentimentScore = []\noverallSentimentType = []\nhighestEmotion = []\nhighestEmotionScore = []\nkywords = []\nentities = []\n\n# Go thru every reponse and enrich the text using NLU\nfor idx, response in enumerate(free_form_responses):\n    #print(\"Processing record number: \", idx, \" and text: \", response)\n    try:\n        enriched_json = json.loads(json.dumps(nlu.analyze(text=response, features=f)))\n        #print(enriched_json)\n\n        # get the SENTIMENT score and type\n        if 'sentiment' in enriched_json:\n            if('score' in enriched_json['sentiment'][\"document\"]):\n                overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n            else:\n                overallSentimentScore.append('0')\n\n            if('label' in enriched_json['sentiment'][\"document\"]):\n                overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n            else:\n                overallSentimentType.append('0')\n\n        # read the EMOTIONS into a dict and get the key (emotion) with maximum value\n        if 'emotion' in enriched_json:\n            me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n            highestEmotion.append(me)\n            highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n\n        else:\n            highestEmotion.append(\"\")\n            highestEmotionScore.append(\"\")\n\n        #iterate and get KEYWORDS with a confidence of over 50%\n        if 'keywords' in enriched_json:\n            #print((enriched_json['keywords']))\n            tmpkw = []\n            for kw in enriched_json['keywords']:\n                if(float(kw[\"relevance\"]) >= 0.5):\n                    #print(\"kw is: \", kw, \"and val is \", kw[\"text\"])\n                    tmpkw.append(kw[\"text\"])#str(kw[\"text\"]).strip('[]')\n            #convert multiple keywords in a list to a string\n            if(len(tmpkw) > 1):\n                tmpkw = \"\".join(reduce(lambda a, b: a + ', ' + b, tmpkw))\n            elif(len(tmpkw) == 0):\n                tmpkw = \"\"\n            else:\n                tmpkw = \"\".join(reduce(lambda a, b='': a + b , tmpkw))\n            kywords.append(tmpkw)\n        else:\n            kywords.append(\"\")\n            \n        #iterate and get Entities with a confidence of over 30%\n        if 'entities' in enriched_json:\n            #print((enriched_json['entities']))\n            tmpent = []\n            for ent in enriched_json['entities']:\n                \n                if(float(ent[\"relevance\"]) >= 0.3):\n                    tmpent.append(ent[\"type\"])\n            #convert multiple concepts in a list to a string\n            if(len(tmpent) > 1):\n                tmpent = \"\".join(reduce(lambda a, b: a + ', ' + b, tmpent))\n            elif(len(tmpent) == 0):\n                tmpent = \"\"\n            else:\n                tmpent = \"\".join(reduce(lambda a, b='': a + b , tmpent))\n            entities.append(tmpent)\n        else:\n            entities.append(\"\")    \n            \n    except:\n        # catch *all* exceptions\n        e = sys.exc_info()[0]\n        overallSentimentScore.append(' ')\n        overallSentimentType.append(' ')\n        highestEmotion.append(' ')\n        highestEmotionScore.append(' ')\n        kywords.append(' ')\n        entities.append(' ')\n        pass\n    \n# Create columns from the list and append to the dataframe\nif highestEmotion:\n    df['TextHighestEmotion'] = highestEmotion\nif highestEmotionScore:\n    df['TextHighestEmotionScore'] = highestEmotionScore\n\nif overallSentimentType:\n    df['TextOverallSentimentType'] = overallSentimentType\nif overallSentimentScore:\n    df['TextOverallSentimentScore'] = overallSentimentScore\n\ndf['TextKeywords'] = kywords\ndf['TextEntities'] = entities", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "After we extract all of the Keywords and Entities from each Post, we have a column with multiple Keywords, and Entities separated by commas. For our Analysis in Part II we wanted also wanted the top Keyword and Entity for each Post. Because of this, we added two new columns to capture the `MaxTextKeyword` and `MaxTextEntity`", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#choose first of Keywords,Concepts, Entities\ndf[\"MaxTextKeywords\"] = df[\"TextKeywords\"].apply(lambda x: x.split(',')[0])\ndf[\"MaxTextEntity\"] = df[\"TextEntities\"].apply(lambda x: x.split(',')[0])\n#df.head()", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id='nlutn'></a>\n###  4.2 NLU for Thumbnail Text\n\nWe will repeat the same process for Thumbnails and Article Text.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Extract the thumbnail text from the data frame\n# If you are using this script for a diff CSV, you will have to change this column name\nfree_form_responses= df['Thumbnails']\n# define the list of enrichments to apply\n# if you are modifying this script add or remove the enrichments as needed\nf = [features.Entities(), features.Keywords(),features.Emotion(),features.Sentiment()]#'typed-rels'\n\n# Create a list to store the enriched data\noverallSentimentScore = []\noverallSentimentType = []\nhighestEmotion = []\nhighestEmotionScore = []\nkywords = []\nentities = []\n\n\n# Go thru every reponse and enrich the text using NLU\nfor idx, response in enumerate(free_form_responses):\n    #print(\"Processing record number: \", idx, \" and text: \", response)\n    try:\n        enriched_json = json.loads(json.dumps(nlu.analyze(text=response, features=f)))\n        #print(enriched_json)\n\n        # get the SENTIMENT score and type\n        if 'sentiment' in enriched_json:\n            if('score' in enriched_json['sentiment'][\"document\"]):\n                overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n            else:\n                overallSentimentScore.append(\"\")\n\n            if('label' in enriched_json['sentiment'][\"document\"]):\n                overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n            else:\n                overallSentimentType.append(\"\")\n\n        # read the EMOTIONS into a dict and get the key (emotion) with maximum value\n        if 'emotion' in enriched_json:\n            me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n            highestEmotion.append(me)\n            highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n\n        else:\n            highestEmotion.append(\"\")\n            highestEmotionScore.append(\"\")\n\n        #iterate and get KEYWORDS with a confidence of over 50%\n        if 'keywords' in enriched_json:\n            #print((enriched_json['keywords']))\n            tmpkw = []\n            for kw in enriched_json['keywords']:\n                if(float(kw[\"relevance\"]) >= 0.5):\n                    #print(\"kw is: \", kw, \"and val is \", kw[\"text\"])\n                    tmpkw.append(kw[\"text\"])#str(kw[\"text\"]).strip('[]')\n            #convert multiple keywords in a list to a string\n            if(len(tmpkw) > 1):\n                tmpkw = \"\".join(reduce(lambda a, b: a + ', ' + b, tmpkw))\n            elif(len(tmpkw) == 0):\n                tmpkw = \"\"\n            else:\n                tmpkw = \"\".join(reduce(lambda a, b='': a + b , tmpkw))\n            kywords.append(tmpkw)\n\n            \n        #iterate and get Entities with a confidence of over 30%\n        if 'entities' in enriched_json:\n            #print((enriched_json['entities']))\n            tmpent = []\n            for ent in enriched_json['entities']:\n                \n                if(float(ent[\"relevance\"]) >= 0.3):\n                    tmpent.append(ent[\"type\"])\n            #convert multiple concepts in a list to a string\n            if(len(tmpent) > 1):\n                tmpent = \"\".join(reduce(lambda a, b: a + ', ' + b, tmpent))\n            elif(len(tmpent) == 0):\n                tmpent = \"\"\n            else:\n                tmpent = \"\".join(reduce(lambda a, b='': a + b , tmpent))\n            entities.append(tmpent)\n        else:\n            entities.append(\"\")  \n    \n            \n    except:\n        # catch *all* exceptions\n        e = sys.exc_info()[0]\n        overallSentimentScore.append(' ')\n        overallSentimentType.append(' ')\n        highestEmotion.append(' ')\n        highestEmotionScore.append(' ')\n        kywords.append(' ')\n        entities.append(' ')\n        pass\n\n# print(len(highestEmotion))\n# print(len(highestEmotionScore))\n# print(len(overallSentimentType))\n# print(len(overallSentimentScore))\n# print(len(kywords))\n# print(len(entities))\n    \n# Create columns from the list and append to the dataframe\nif highestEmotion:\n    df['ThumbnailHighestEmotion'] = highestEmotion\nif highestEmotionScore:\n    df['ThumbnailHighestEmotionScore'] = highestEmotionScore\n\nif overallSentimentType:\n    df['ThumbnailOverallSentimentType'] = overallSentimentType\nif overallSentimentScore:\n    df['ThumbnailOverallSentimentScore'] = overallSentimentScore\n\ndf['ThumbnailKeywords'] = kywords\ndf['ThumbnailEntities'] = entities", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "#choose first of Keywords,Concepts,Entities\ndf[\"MaxThumbnailKeywords\"] = df[\"ThumbnailKeywords\"].apply(lambda x: x.split(',')[0])\ndf[\"MaxThumbnailEntity\"] = df[\"ThumbnailEntities\"].apply(lambda x: x.split(',')[0])\n#df.head()", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id='nlulink'></a> \n### 4.3 NLU for Article Text", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Run links through NLU and return Titles, and NLU Enrichment on full articles\n# If you are using this script for a diff CSV, you will have to change this column name\nfree_form_responses = df['Extended Links']\n# define the list of enrichments to apply\n# if you are modifying this script add or remove the enrichments as needed\nf = [features.Entities(), features.Keywords(),features.Emotion(),features.Sentiment()]#'typed-rels'\n\n# Create a list to store the enriched data\noverallSentimentScore = []\noverallSentimentType = []\nhighestEmotion = []\nhighestEmotionScore = []\nkywords = []\nentities = []\n\n\n\n        \n# Go thru every reponse and enrich the text using NLU\nfor idx, response in enumerate(free_form_responses):\n    #print(\"Processing record number: \", idx, \" and text: \", response)\n    try:\n        enriched_json = json.loads(json.dumps(nlu.analyze(url=response, features=f)))\n        #print(enriched_json)\n\n        # get the SENTIMENT score and type\n        if 'sentiment' in enriched_json:\n            if('score' in enriched_json['sentiment'][\"document\"]):\n                overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n            else:\n                overallSentimentScore.append('None')\n\n            if('label' in enriched_json['sentiment'][\"document\"]):\n                overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n            else:\n                overallSentimentType.append('')\n\n        # read the EMOTIONS into a dict and get the key (emotion) with maximum value\n        if 'emotion' in enriched_json:\n            me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n            highestEmotion.append(me)\n            highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n\n        else:\n            highestEmotion.append('')\n            highestEmotionScore.append('')\n\n        #iterate and get KEYWORDS with a confidence of over 50%\n        if 'keywords' in enriched_json:\n            #print((enriched_json['keywords']))\n            tmpkw = []\n            for kw in enriched_json['keywords']:\n                if(float(kw[\"relevance\"]) >= 0.5):\n                    #print(\"kw is: \", kw, \"and val is \", kw[\"text\"])\n                    tmpkw.append(kw[\"text\"])#str(kw[\"text\"]).strip('[]')\n            #convert multiple keywords in a list to a string\n            if(len(tmpkw) > 1):\n                tmpkw = \"\".join(reduce(lambda a, b: a + ', ' + b, tmpkw))\n            elif(len(tmpkw) == 0):\n                tmpkw = \"\"\n            else:\n                tmpkw = \"\".join(reduce(lambda a, b='': a + b , tmpkw))\n            kywords.append(tmpkw)\n        else: \n            kywords.append(\"\")\n            \n        #iterate and get Entities with a confidence of over 30%\n        if 'entities' in enriched_json:\n            #print((enriched_json['entities']))\n            tmpent = []\n            for ent in enriched_json['entities']:\n                \n                if(float(ent[\"relevance\"]) >= 0.3):\n                    tmpent.append(ent[\"type\"])\n            #convert multiple concepts in a list to a string\n            if(len(tmpent) > 1):\n                tmpent = \"\".join(reduce(lambda a, b: a + ', ' + b, tmpent))\n            elif(len(tmpent) == 0):\n                tmpent = \"\"\n            else:\n                tmpent = \"\".join(reduce(lambda a, b='': a + b , tmpent))\n            entities.append(tmpent)\n        else:\n            entities.append(\"\")\n    \n            \n    except:\n        # catch *all* exceptions\n        e = sys.exc_info()[0]\n        overallSentimentScore.append(' ')\n        overallSentimentType.append(' ')\n        highestEmotion.append(' ')\n        highestEmotionScore.append(' ')\n        kywords.append(' ')\n#       concepts.append(' ')\n        entities.append(' ')\n\n        pass\n    \n# Create columns from the list and append to the dataframe\nif highestEmotion:\n    df['LinkHighestEmotion'] = highestEmotion\nif highestEmotionScore:\n    df['LinkHighestEmotionScore'] = highestEmotionScore\n\nif overallSentimentType:\n    df['LinkOverallSentimentType'] = overallSentimentType\nif overallSentimentScore:\n    df['LinkOverallSentimentScore'] = overallSentimentScore\n\ndf['LinkKeywords'] = kywords\n# df['TextConcepts'] = concepts\ndf['LinkEntities'] = entities\n", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "df[\"MaxLinkKeywords\"] = df[\"LinkKeywords\"].apply(lambda x: x.split(',')[0])\ndf[\"MaxLinkEntity\"] = df[\"LinkEntities\"].apply(lambda x: x.split(',')[0])\n#df.head()", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": " <a id='tonepost'></a> \n### 4.4 Tone Analyzer for Post Text", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Extract the free form text response from the data frame\n# If you are using this script for a diff CSV, you will have to change this column name\nfree_form_responses = df['Text']\n\n#Create a list to store the enriched data\n\nhighestEmotionTone = []\nemotionToneScore = []\n\nlanguageToneScore = []\nhighestLanguageTone = []\n\nsocialToneScore = []\nhighestSocialTone = []\n\n\nfor idx, response in enumerate(free_form_responses):\n    #print(\"Processing record number: \", idx, \" and text: \", response)\n    try:\n        enriched_json = json.loads(json.dumps(tone_analyzer.tone(text=response)))\n        #print(enriched_json)\n        \n        if 'tone_categories' in enriched_json['document_tone']:\n            me = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['tone_name']      \n            highestEmotionTone.append(me)\n            you = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['score']\n            emotionToneScore.append(you)\n            \n            me1 = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['tone_name']      \n            highestLanguageTone.append(me1)\n            you1 = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['score']\n            languageToneScore.append(you1)\n            \n            me2 = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['tone_name']      \n            highestSocialTone.append(me2)\n            you2 = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['score']\n            socialToneScore.append(you2)\n            \n            \n            \n    except:\n        # catch *all* exceptions\n        e = sys.exc_info()[0]\n        emotionToneScore.append(' ')\n        highestEmotionTone.append(' ')\n        languageToneScore.append(' ')\n        highestLanguageTone.append(' ')\n        socialToneScore.append(' ')\n        highestSocialTone.append(' ')\n        pass\n    \nif highestEmotionTone:\n    df['highestEmotionTone'] = highestEmotionTone    \nif emotionToneScore:\n    df['emotionToneScore'] = emotionToneScore\n    \nif languageToneScore:\n    df['languageToneScore'] = languageToneScore\nif highestLanguageTone:\n    df['highestLanguageTone'] = highestLanguageTone\n    \nif highestSocialTone:\n    df['highestSocialTone'] = highestSocialTone    \nif socialToneScore:\n    df['socialToneScore'] = socialToneScore \n    \n#df.head()", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id='enrich2'></a> \n### 4.5 Tone Analyzer for Article Text", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "Unlike NLU, Tone Analyzer cannot iterate through a URL so here we use NLU to pull the Article Text from the URL and append it to the original dataframe. \n\nTo do this, we pull out the `MetaData` feature, and make sure the `return_analyzed_text` parameter is set to `True`. ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Extract the free form text response from the data frame\n# If you are using this script for a diff CSV, you will have to change this column name\nfree_form_responses = df['Link']\n# define the list of enrichments to apply\n# if you are modifying this script add or remove the enrichments as needed\nf = [features.MetaData()]#'typed-rels'\narticle_text = []\n\n\nfor idx, response in enumerate(free_form_responses):\n    try:\n        enriched_json = json.loads(json.dumps(nlu.analyze(url=response, features=f,return_analyzed_text=True)))\n        #print(enriched_json)\n        article_text.append(enriched_json[\"analyzed_text\"])\n    except:\n    \n        article_text.append(\"\")\n        \n#save to dataframe\ndf[\"Article Text\"] = article_text\n#df.head()", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "Similar to the script for NLU, we are now using Tone Analyzer to iterate through the newly created and appended `Article Text` column which contains all of the free form text from the articles contained in the Facebook posts. \n\nWe are using Tone Analyzer to gather the tope Social, Writing and Emotion Tones from the Articles and appending them, along with their respective scores to the `.csv`", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Extract the free form text response from the data frame\n# If you are using this script for a diff CSV, you will have to change this column name\nfree_form_responses = df['Article Text']\n\n#Create a list to store the enriched data\n\nhighestEmotionTone = []\nemotionToneScore = []\n\nlanguageToneScore = []\nhighestLanguageTone = []\n\nsocialToneScore = []\nhighestSocialTone = []\n\n\nfor idx, response in enumerate(free_form_responses):\n    #print(\"Processing record number: \", idx, \" and text: \", response)\n    try:\n        enriched_json = json.loads(json.dumps(tone_analyzer.tone(text=response)))\n        #print(enriched_json)\n        \n        if 'tone_categories' in enriched_json['document_tone']:\n            maxTone = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['tone_name']      \n            highestEmotionTone.append(maxTone)\n            maxToneScore = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['score']\n            emotionToneScore.append(maxToneScore)\n            \n            maxLanguageTone = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['tone_name']      \n            highestLanguageTone.append(maxLanguageTone)\n            maxLanguageScore = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['score']\n            languageToneScore.append(maxLanguageScore)\n            \n            maxSocial = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['tone_name']      \n            highestSocialTone.append(maxSocial)\n            maxSocialScore = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['score']\n            socialToneScore.append(maxSocialScore)\n            \n    except:\n        # catch *all* exceptions\n        e = sys.exc_info()[0]\n        emotionToneScore.append(' ')\n        highestEmotionTone.append(' ')\n        \n        languageToneScore.append(' ')\n        highestLanguageTone.append(' ')\n        \n        socialToneScore.append(' ')\n        highestSocialTone.append(' ')\n        \n        pass\n    \nif highestEmotionTone:\n    df['articlehighestEmotionTone'] = highestEmotionTone    \nif emotionToneScore:\n    df['articleEmotionToneScore'] = emotionToneScore  \nif languageToneScore:\n    df['articlelanguageToneScore'] = languageToneScore\nif highestLanguageTone:\n    df['articlehighestLanguageTone'] = highestLanguageTone   \nif highestSocialTone:\n    df['articlehighestSocialTone'] = highestSocialTone    \nif socialToneScore:\n    df['articlesocialToneScore'] = socialToneScore \n\n#df.head()", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "df.head()", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id='visual'></a> \n### 4.6 Visual Recognition ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "piclinks = df[\"Image\"]\n\npicclass = []\n\nfor pic in enumerate(piclinks):\n    try:\n        enriched_json = json.loads(json.dumps(visual_recognition.classify(images_url=pic), indent=2))\n        #print(enriched_json)\n        classes = enriched_json['images'][0][\"classifiers\"][0][\"classes\"]\n        length = len(classes)\n        tpicclass = []\n        #for each class within one picture\n        for n in range(0,length):\n            #iclass is one class\n            iclass = classes[n]\n            #for confidence level .70\n            if float(iclass[\"score\"]>=.70):\n                tpicclass.append(iclass[\"class\"]) \n            \n        if(len(tpicclass) > 1):\n            tpicclass = \"\".join(reduce(lambda a, b: a + ', ' + b, tpicclass))\n        elif(len(tpicclass) == 0):\n            tpicclass = \"\"\n        else:\n            tpicclass = \"\".join(reduce(lambda a, b: a + ', ' + b, tpicclass))\n\n        picclass.append(tpicclass)\n        \n    except:\n        # catch *all* exceptions\n        e = sys.exc_info()[0]\n        picclass.append(' ')\n        pass\n\ndf[\"PicClass\"] = picclass", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": " <a id='write'></a>\n## Enrichment is now COMPLETE!\n<a id='write1'></a> \nLast step is to write and save the enriched dataframe to SoftLayer's Object Storage.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "Since we already created the `localfilename` variable in the Setup stage and defined the necessary credentials, this snippet will work for all new files and does not need to be changed.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "def put_file(credentials, local_file_name):  \n    \"\"\"This functions returns a StringIO object containing\n    the file content from Bluemix Object Storage V3.\"\"\"\n    f = open(local_file_name,'r',encoding=\"utf-8\")\n    my_data = f.read()\n    data_to_send = my_data.encode(\"utf-8\")\n    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n            'password': {'user': {'name': credentials['username'],'domain': {'id': credentials['domain_id']},\n            'password': credentials['password']}}}}}\n    headers1 = {'Content-Type': 'application/json'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()\n    #print(resp1_body)\n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                        if(e2['interface']=='public'and e2['region']=='dallas'):\n                            url2 = ''.join([e2['url'],'/', credentials['container'], '/',  local_file_name])\n                            print(url2)\n    s_subject_token = resp1.headers['x-subject-token']\n    #print(s_subject_token)\n    #print(credentials['container'])\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.put(url=url2, headers=headers2, data = data_to_send )\n    print(resp2)", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "#choose any name to save your file\ndf.to_csv(localfilename,index=False)", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id='write2'></a> Make sure to change the \"credential\" argument below matches the variable name of the credentials you imported in the Setup Phase.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "put_file(credentials_1,localfilename)", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"part2\"></a> \n# Part II - Data Preparation\n<a id='prepare'></a>\n## 1. Prepare Data\n <a id='visualizations'></a>\n### 1.1 Prepare Multiple DataFrames for Visualizations\nBefore we can create the separate tables for each Watson feature we need to organize and reformat the data. First, we need to determine which data points are tied to metrics. Second, we need to make sure make sure each metric is numeric. _(This is necessary for PixieDust in Part III)_", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#Determine which data points are tied to metrics and put them in a list\nmetrics = [\"Lifetime Post Total Reach\", \"Lifetime Post organic reach\", \"Lifetime Post Paid Reach\", \"Lifetime Post Total Impressions\", \"Lifetime Post Organic Impressions\", \n           \"Lifetime Post Paid Impressions\", \"Lifetime Engaged Users\", \"Lifetime Post Consumers\", \"Lifetime Post Consumptions\", \"Lifetime Negative feedback\", \"Lifetime Negative Feedback from Users\", \n           \"Lifetime Post Impressions by people who have liked your Page\", \"Lifetime Post reach by people who like your Page\", \"Lifetime Post Paid Impressions by people who have liked your Page\", \n           \"Lifetime Paid reach of a post by people who like your Page\", \"Lifetime People who have liked your Page and engaged with your post\", \"Lifetime Talking About This (Post) by action type - comment\", \n           \"Lifetime Talking About This (Post) by action type - like\", \"Lifetime Talking About This (Post) by action type - share\", \"Lifetime Post Stories by action type - comment\", \"Lifetime Post Stories by action type - like\", \n           \"Lifetime Post Stories by action type - share\", \"Lifetime Post consumers by type - link clicks\", \"Lifetime Post consumers by type - other clicks\", \"Lifetime Post consumers by type - photo view\", \"Lifetime Post Consumptions by type - link clicks\", \n           \"Lifetime Post Consumptions by type - other clicks\", \"Lifetime Post Consumptions by type - photo view\", \"Lifetime Negative feedback - hide_all_clicks\", \"Lifetime Negative feedback - hide_clicks\", \n           \"Lifetime Negative Feedback from Users by Type - hide_all_clicks\", \"Lifetime Negative Feedback from Users by Type - hide_clicks\"]\n\n", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id='tone'></a> \n### 1.2 Create A Consolidated Tone Dataframe", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Post Tone Dataframe", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#Create a list with only Post Tone Values\npost_tones = [\"Text\",\"highestEmotionTone\", \"emotionToneScore\", \"languageToneScore\", \"highestLanguageTone\", \"highestSocialTone\", \"socialToneScore\"]\n\n#Append dataframe with these metrics\npost_tones.extend(metrics)\n\n#Create a new dataframe with tones and metrics\ndf_post_tones = df[post_tones]\n\n#Determine which tone values are suppose to be numeric and ensure they are numeric. \npost_numeric_values = [\"emotionToneScore\", \"languageToneScore\", \"socialToneScore\"]\nfor i in post_numeric_values:\n    df_post_tones[i] = pd.to_numeric(df_post_tones[i], errors='coerce')\n\n#Make all metrics numeric\nfor i in metrics:\n    df_post_tones[i] = pd.to_numeric(df_post_tones[i], errors='coerce')\n    \n#Drop NA Values in Tone Enrichment Columns\ndf_post_tones.dropna(subset=[\"socialToneScore\"] , inplace = True)\n\n\n#Add in a column to distinguish what portion the enrichment was happening \ndf_post_tones[\"Type\"] = \"Post\"\n\n#df_post_tones.info()", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Article Tone Dataframe", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#Create a list with only Article Tone Values\narticle_tones = [\"Text\", \"articlehighestEmotionTone\", \"articleEmotionToneScore\", \"articlelanguageToneScore\", \"articlehighestLanguageTone\", \"articlehighestSocialTone\", \"articlesocialToneScore\"]\n\n#Append dataframe with these metrics\narticle_tones.extend(metrics)\n\n#Create a new dataframe with tones and metrics\ndf_article_tones = df[article_tones]\n\n#Determine which values are suppose to be numeric and ensure they are numeric. \nart_numeric_values = [\"articleEmotionToneScore\", \"articlelanguageToneScore\", \"articlesocialToneScore\"]\nfor i in art_numeric_values:\n    df_article_tones[i] = pd.to_numeric(df_article_tones[i], errors='coerce')\n    \n#Make all metrics numeric\nfor i in metrics:\n    df_article_tones[i] = pd.to_numeric(df_article_tones[i], errors='coerce')\n    \n#Drop NA Values in Tone Enrichment Columns\ndf_article_tones.dropna(subset=[\"articlesocialToneScore\"] , inplace = True)\n\n#Add in a column to distinguish what portion the enrichment was happening \ndf_article_tones[\"Type\"] = \"Article\"\n\n#df_article_tones.head()", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Combine Post and Article Dataframes to Make One Tone Dataframe", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#first make the Column Headers the same\ndf_post_tones.rename(columns={\"highestEmotionTone\":\"Emotion Tone\", \"emotionToneScore\":\"Emotion Tone Score\", \"languageToneScore\": \"Language Tone Score\", \"highestLanguageTone\": \"Language Tone\", \"highestSocialTone\": \"Social Tone\", \"socialToneScore\":\"Social Tone Score\"\n}, inplace=True)\n\ndf_article_tones.rename(columns={\"articlehighestEmotionTone\":\"Emotion Tone\", \"articleEmotionToneScore\":\"Emotion Tone Score\", \"articlelanguageToneScore\": \"Language Tone Score\", \"articlehighestLanguageTone\": \"Language Tone\", \"articlehighestSocialTone\": \"Social Tone\", \"articlesocialToneScore\":\"Social Tone Score\"\n}, inplace=True)\n\n#Combine into one data frame\ndf_tones = pd.concat([df_post_tones, df_article_tones])\n\n#df_tones.head()", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": " <a id='keyword'></a> \n### 1.3 Create A Consolidated Keyword Dataframe", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": " #### Article Keyword Dataframe ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#Create a list with only Article Keywords\narticle_keywords = [\"Text\", \"MaxLinkKeywords\"]\n\n#Append dataframe with these metrics\narticle_keywords.extend(metrics)\n\n#Create a new dataframe with keywords and metrics\ndf_article_keywords = df[article_keywords]\n\n#Make all metrics numeric\nfor i in metrics:\n    df_article_keywords[i] = pd.to_numeric(df_article_keywords[i], errors='coerce')\n  \n#Drop NA Values in Keywords Column\n\ndf_article_keywords['MaxLinkKeywords'].replace(' ', np.nan, inplace=True)\ndf_article_keywords.dropna(subset=['MaxLinkKeywords'], inplace=True)\n\n#Add in a column to distinguish what portion the enrichment was happening \ndf_article_keywords[\"Type\"] = \"Article\"\n\n#df_article_keywords.head()", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Thumbnail Keyword Dataframe ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#Create a list with only Thumbnail Keywords\nthumbnail_keywords = [\"Text\", \"MaxThumbnailKeywords\"]\n\n#Append dataframe with these metrics\nthumbnail_keywords.extend(metrics)\n\n#Create a new dataframe with keywords and metrics\ndf_thumbnail_keywords = df[thumbnail_keywords]\n\n\n#Make all metrics numeric\nfor i in metrics:\n    df_thumbnail_keywords[i] = pd.to_numeric(df_thumbnail_keywords[i], errors='coerce')\n    \n#Drop NA Values in Keywords Column\n\ndf_thumbnail_keywords['MaxThumbnailKeywords'].replace(' ', np.nan, inplace=True)\ndf_thumbnail_keywords.dropna(subset=['MaxThumbnailKeywords'], inplace=True)\n\n#Add in a column to distinguish what portion the enrichment was happening \ndf_thumbnail_keywords[\"Type\"] = \"Thumbnails\"\n\n#df_thumbnail_keywords.head()", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Post Keyword Dataframe ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#Create a list with only Thumbnail Keywords\npost_keywords = [\"Text\", \"MaxTextKeywords\"]\n\n#Append dataframe with these metrics\npost_keywords.extend(metrics)\n\n#Create a new dataframe with keywords and metrics\ndf_post_keywords = df[post_keywords]\n\n#Make all metrics numeric\nfor i in metrics:\n    df_post_keywords[i] = pd.to_numeric(df_post_keywords[i], errors='coerce')\n    \n#Drop NA Values in Keywords Column\n\ndf_post_keywords['MaxTextKeywords'].replace(' ', np.nan, inplace=True)\ndf_post_keywords.dropna(subset=['MaxTextKeywords'], inplace=True)\n\n#Add in a column to distinguish what portion the enrichment was happening \ndf_post_keywords[\"Type\"] = \"Posts\"\n\n# df_post_keywords.info()", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Combine Post, Thumbnail, and Article Dataframes to Make One Keywords Dataframe", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#first make the Column Headers the same\ndf_post_keywords.rename(columns={\"MaxTextKeywords\": \"Keywords\"}, inplace=True)\n\ndf_thumbnail_keywords.rename(columns={\"MaxThumbnailKeywords\":\"Keywords\"}, inplace=True)\n\ndf_article_keywords.rename(columns={\"MaxLinkKeywords\":\"Keywords\"}, inplace=True)\n\n#Combine into one data frame\ndf_keywords = pd.concat([df_post_keywords, df_thumbnail_keywords, df_article_keywords])\n\ndf_keywords = df_keywords[df_keywords[\"Lifetime Post Consumptions\"]>700]\n#df_keywords", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id='entity'></a>\n###  1.4 Create A Consolidated Entity Dataframe", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Article Entity Dataframe ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#Create a list with only Article Keywords\narticle_entities = [\"Text\", \"MaxLinkEntity\"]\n\n#Append dataframe with these metrics\narticle_entities.extend(metrics)\n\n#Create a new dataframe with keywords and metrics\ndf_article_entities = df[article_entities]\n    \n#Make all metrics numeric\nfor i in metrics:\n    df_article_entities[i] = pd.to_numeric(df_article_entities[i], errors='coerce')\n    \n#Drop NA Values in Keywords Column\n\ndf_article_entities['MaxLinkEntity'] = df[\"MaxLinkEntity\"].replace(r'\\s+', np.nan, regex=True)\ndf_article_entities.dropna(subset=['MaxLinkEntity'], inplace=True)\n\n#Add in a column to distinguish what portion the enrichment was happening \ndf_article_entities[\"Type\"] = \"Article\"\n\n#df_article_entities", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Thumbnail Entity Dataframe", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#Create a list with only Thumbnail Keywords\nthumbnail_entities = [\"Text\", \"MaxThumbnailEntity\"]\n\n#Append dataframe with these metrics\nthumbnail_entities.extend(metrics)\n\n#Create a new dataframe with keywords and metrics\ndf_thumbnail_entities = df[thumbnail_entities]\n\n#Make all metrics numeric\nfor i in metrics:\n    df_thumbnail_entities[i] = pd.to_numeric(df_thumbnail_entities[i], errors='coerce')\n    \n#Drop NA Values in Keywords Column\n\ndf_thumbnail_entities['MaxThumbnailEntity'] = df_thumbnail_entities['MaxThumbnailEntity'].replace(r'\\s+', np.nan, regex=True)\ndf_thumbnail_entities.dropna(subset=['MaxThumbnailEntity'], inplace=True)\n\n#Add in a column to distinguish what portion the enrichment was happening \ndf_thumbnail_entities[\"Type\"] = \"Thumbnails\"\n\n#df_thumbnail_entities", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Post Entity Dataframe", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#Create a list with only Thumbnail Keywords\npost_entities = [\"Text\", \"MaxTextEntity\"]\n\n#Append dataframe with these metrics\npost_entities.extend(metrics)\n\n#Create a new dataframe with keywords and metrics\ndf_post_entities = df[post_entities]\n\n#Make all metrics numeric\nfor i in metrics:\n    df_post_entities[i] = pd.to_numeric(df_post_entities[i], errors='coerce')\n    \n#Drop NA Values in Keywords Column\n\ndf_post_entities['MaxTextEntity'] = df_post_entities['MaxTextEntity'].replace(r'\\s+', np.nan, regex=True)\ndf_post_entities.dropna(subset=['MaxTextEntity'], inplace=True)\n\n#Add in a column to distinguish what portion the enrichment was happening \ndf_post_entities[\"Type\"] = \"Posts\"\n\n#df_post_entities", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Combine Post, Thumbnail, and Article Dataframes to Make One Entity Dataframe", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#first make the Column Headers the same\ndf_post_entities.rename(columns={\"MaxTextEntity\": \"Entities\"}, inplace=True)\n\ndf_thumbnail_entities.rename(columns={\"MaxThumbnailEntity\":\"Entities\"}, inplace=True)\n\ndf_article_entities.rename(columns={\"MaxLinkEntity\":\"Entities\"}, inplace=True)\n\n\n\n#Combine into one data frame\ndf_entities = pd.concat([df_post_entities, df_thumbnail_entities, df_article_entities])\n\ndf_entities[\"Entities\"] = df_entities[\"Entities\"].replace('', np.nan)\ndf_entities.dropna(subset=[\"Entities\"], inplace=True)\n\n\n#df_entities", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"part3\"></a> \n# Part III\n<a id='2setup'></a> \n## 1. Setup\n<a id='2setup2'></a>\n###  1.1 Assign Variables\nAssign new dataframes to variables. ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "entities = df_entities\ntones = df_tones\nkeywords = df_keywords", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=''></a>\n##  2. Visualize Data\n<a id=''></a> \n### 2.1 Run PixieDust Visualization Library with Display() API\nPixieDust lets you visualize your data in just a few clicks using the display() API. You can find more info at https://ibm-cds-labs.github.io/pixiedust/displayapi.html. The following cell creates a DataFrame and uses the display() API to create a bar chart:", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### The first thing we can do is see how Lifetime Post Consumption is related to emotion tone. Clicking the \"Options\" icon allows you to change the metrics. ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "display(tones)", 
            "metadata": {
                "scrolled": false, 
                "pixiedust": {
                    "displayParams": {
                        "chartsize": "71", 
                        "aggregation": "COUNT", 
                        "keyFields": "Emotion Tone", 
                        "legend": "true", 
                        "valueFields": "Lifetime Post Consumptions", 
                        "handlerId": "barChart", 
                        "rowCount": "100", 
                        "rendererId": "matplotlib"
                    }
                }
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### We can use also use a pie chart to identify how post consumption was broken up by tone. ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "display(tones)", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "keyFields": "Emotion Tone", 
                        "chartsize": "58", 
                        "aggregation": "SUM", 
                        "valueFields": "Lifetime Post Consumptions", 
                        "handlerId": "pieChart", 
                        "rowCount": "100"
                    }
                }
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### We can find out how mean post clicks differed by entity. ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "display(entities)", 
            "metadata": {
                "scrolled": false, 
                "pixiedust": {
                    "displayParams": {
                        "aggregation": "SUM", 
                        "orientation": "vertical", 
                        "keyFields": "Entities", 
                        "charttype": "grouped", 
                        "valueFields": "Lifetime Post Consumptions by type - other clicks", 
                        "handlerId": "barChart", 
                        "rowCount": "100", 
                        "clusterby": "Type", 
                        "rendererId": "matplotlib"
                    }
                }
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Finally we can see how post consumption was associated with certain keywords. ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "display(keywords)", 
            "metadata": {
                "scrolled": false, 
                "pixiedust": {
                    "displayParams": {
                        "mpld3": "false", 
                        "chartsize": "72", 
                        "aggregation": "SUM", 
                        "orientation": "horizontal", 
                        "keyFields": "Keywords", 
                        "valueFields": "Lifetime Post consumers by type - link clicks", 
                        "handlerId": "barChart", 
                        "rowCount": "100", 
                        "clusterby": "Type", 
                        "sortby": "Keys DESC", 
                        "rendererId": "matplotlib"
                    }
                }
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "source": "# More Info.\nFor more information about PixieDust check out the following:\n#### PixieDust Documentation: https://ibm-cds-labs.github.io/pixiedust/index.html\n#### PixieDust GitHub Repo: https://github.com/ibm-cds-labs/pixiedust", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Authors\n\nTyler Andersen, Anna Quincy - From the Watson Accelerator\u2019s Team, Anna and Tyler specialize in combining the power of the Watson Services and Watson Data Platform.", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Copyright \u00a9 2017 IBM. This notebook and its source code are released under the terms of the MIT License.", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "nbformat_minor": 1, 
    "metadata": {
        "kernelspec": {
            "language": "python", 
            "name": "python3-spark20", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.0"
        }, 
        "language_info": {
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "file_extension": ".py", 
            "version": "3.5.2", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "name": "python"
        }
    }, 
    "nbformat": 4
}